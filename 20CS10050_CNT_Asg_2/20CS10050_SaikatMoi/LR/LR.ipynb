{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Node2Vec and Logistic Regression\n",
    "## Assignment 2\n",
    "### Name: Saikat Moi\n",
    "### Roll Number: 20CS10050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing nessesary Libraries \n",
    "\n",
    "import networkx as nx # to visulaize and load graph data\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec # for implimenting Skip-Gram Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph: Accepts a graph as input.\n",
    "# probs: Requires an empty dictionary to calculate probabilities for all neighboring nodes of a given node.\n",
    "# p: Returns a parameter.\n",
    "# q: Represents an in-out parameter.\n",
    "\n",
    "def compute_probabilities(graph, probs, p, q):\n",
    "    G = graph\n",
    "    for source_node in G.nodes():\n",
    "        for current_node in G.neighbors(source_node):\n",
    "            probs_ = list()\n",
    "            for destination in G.neighbors(current_node):\n",
    "\n",
    "                if source_node == destination:\n",
    "                    prob_ = G[current_node][destination].get('weight',1) * (1/p)\n",
    "                elif destination in G.neighbors(source_node):\n",
    "                    prob_ = G[current_node][destination].get('weight',1)\n",
    "                else:\n",
    "                    prob_ = G[current_node][destination].get('weight',1) * (1/q)\n",
    "\n",
    "                probs_.append(prob_)\n",
    "\n",
    "            probs[source_node]['probabilities'][current_node] = probs_/np.sum(probs_)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph: Requires a graph as input.\n",
    "# probs: Indicates computed probabilities.\n",
    "# max_walks: Specifies the maximum number of walks allowed per node.\n",
    "# walk_len: Denotes the maximum length of each walk.\n",
    "\n",
    "\n",
    "def generate_random_walks(graph,probs,max_walks, walk_len):\n",
    "\n",
    "    \n",
    "    G = graph\n",
    "    walks = list()\n",
    "    for start_node in G.nodes():\n",
    "        for i in range(max_walks):\n",
    "            \n",
    "            walk = [start_node]\n",
    "            walk_options = list(G[start_node])\n",
    "            if len(walk_options)==0:\n",
    "                break\n",
    "            first_step = np.random.choice(walk_options)\n",
    "            walk.append(first_step)\n",
    "            \n",
    "            for k in range(walk_len-2):\n",
    "                walk_options = list(G[walk[-1]])\n",
    "                if len(walk_options)==0:\n",
    "                    break\n",
    "                probabilities = probs[walk[-2]]['probabilities'][walk[-1]]\n",
    "                next_step = np.random.choice(walk_options, p=probabilities)\n",
    "                walk.append(next_step)\n",
    "            \n",
    "            walks.append(walk)\n",
    "    np.random.shuffle(walks)\n",
    "    walks = [list(map(str,walk)) for walk in walks]\n",
    "    \n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node2Vec_impl(generated_walks,window_size,embedding_vector_size):\n",
    "    model = Word2Vec(sentences=generated_walks, window=window_size, vector_size=embedding_vector_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Read .content file to get node features and labels\n",
    "    with open(\"../../dataset/cora.content\", \"r\") as content_file:\n",
    "        content_lines = content_file.readlines()\n",
    "    \n",
    "    # Read .cites files to build the citation graph\n",
    "    train_cites = np.loadtxt(\"../../dataset/cora_train.cites\", dtype=int)\n",
    "    test_cites = np.loadtxt(\"../../dataset/cora_test.cites\", dtype=int)\n",
    "    \n",
    "    # Create a directed graph\n",
    "    citation_graph = nx.DiGraph()\n",
    "    train_graph = nx.DiGraph()\n",
    "    test_graph = nx.DiGraph()\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    for paper1, paper2 in train_cites:\n",
    "        citation_graph.add_edge(paper2, paper1)  # Adding the edge with correct direction\n",
    "        train_graph.add_edge(paper2, paper1)\n",
    "        \n",
    "\n",
    "    \n",
    "    for paper1, paper2 in test_cites:\n",
    "        citation_graph.add_edge(paper2, paper1)  # Adding the edge with correct direction\n",
    "        test_graph.add_edge(paper2, paper1)\n",
    "\n",
    "    \n",
    "    # Extract node features and labels\n",
    "    node_features = []\n",
    "    node_labels = {}\n",
    "    for line in content_lines:\n",
    "        data = line.strip().split()\n",
    "        paper_id = int(data[0])\n",
    "        class_label = data[-1]\n",
    "        node_features.append([int(x) for x in data[1:-1]])\n",
    "        node_labels[paper_id] = class_label       \n",
    "    \n",
    "    return node_features, node_labels, citation_graph,train_graph,test_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features, node_labels, citation_graph,train_graph,test_graph= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = citation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "probs = defaultdict(dict)\n",
    "for node in G.nodes():\n",
    "    probs[node]['probabilities'] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = compute_probabilities(G,probs,1,1)\n",
    "\n",
    "walks = generate_random_walks(G,cp,50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "\n",
    "n2v_emb = Node2Vec_impl(walks,5,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train=[]\n",
    "for node_id in train_graph.nodes():\n",
    "    if str(node_id) in n2v_emb.wv:\n",
    "        embedding = n2v_emb.wv.get_vector(str(node_id))\n",
    "        X_train.append(embedding)\n",
    "        y_train.append(node_labels[node_id])\n",
    "\n",
    "X_test = []\n",
    "y_test=[]\n",
    "\n",
    "for node_id in test_graph.nodes():\n",
    "    if str(node_id) in n2v_emb.wv:\n",
    "        embedding = n2v_emb.wv.get_vector(str(node_id))\n",
    "        X_test.append(embedding)\n",
    "        y_test.append(node_labels[node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7524091919940696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Case_Based       0.73      0.61      0.67       166\n",
      "    Genetic_Algorithms       0.88      0.87      0.88       210\n",
      "       Neural_Networks       0.70      0.85      0.77       366\n",
      " Probabilistic_Methods       0.84      0.79      0.81       207\n",
      "Reinforcement_Learning       0.86      0.71      0.78       128\n",
      "         Rule_Learning       0.69      0.56      0.62        98\n",
      "                Theory       0.62      0.63      0.63       174\n",
      "\n",
      "              accuracy                           0.75      1349\n",
      "             macro avg       0.76      0.72      0.74      1349\n",
      "          weighted avg       0.76      0.75      0.75      1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n",
    "\n",
    "# Write the classification report to a file\n",
    "with open(\"lr_metrics.txt\", \"w\") as file:\n",
    "    file.write(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b504f5870eb083b25ac0ccd73401bf891a86c84f0b0ce8357843d17974c5ef43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
