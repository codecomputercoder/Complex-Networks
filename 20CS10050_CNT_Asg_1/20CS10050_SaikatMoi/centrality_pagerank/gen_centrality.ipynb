{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Name: Saikat Moi\n",
    "### Roll Number: 20CS10050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saikat Moi\\AppData\\Local\\Temp\\ipykernel_4304\\1480632955.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# import all the necessary libraries here\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_array = []\n",
    "nodes=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../cora/cora.cites\", \"r\")\n",
    "for x in f:\n",
    "  node1, node2 = map(int, x.split())\n",
    "  edges_array.append((node1, node2))\n",
    "  nodes.add(node1)\n",
    "  nodes.add(node2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array(edges_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    " For all the nodes which has outdegree 0 , considering it has edges to all the nodes from which it has incoming edges and accordingly creating the adjacency matrix for this problem\n",
    "\n",
    " If you do not find any valid path between two nodes, you can assume a very large value (say 10^9) as the shortest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indices = list(nodes)\n",
    "\n",
    "# Create an adjacency matrix initialized with zeros\n",
    "num_nodes = len(node_indices)\n",
    "adj_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n",
    "outgoing_edges_count = np.zeros(num_nodes, dtype=int)\n",
    "\n",
    "for edge in edges_array:\n",
    "    node1, node2 = edge\n",
    "    index1 = node_indices.index(node1)\n",
    "    outgoing_edges_count[index1] += 1\n",
    "\n",
    "# Populate the adjacency matrix based on the edges\n",
    "for edge in edges_array:\n",
    "    node1, node2 = edge\n",
    "    index1, index2 = node_indices.index(node1), node_indices.index(node2)\n",
    "    adj_matrix[index1, index2] = 1  # Assuming the graph is directed index1->index2\n",
    "    if outgoing_edges_count[index2]==0:\n",
    "        adj_matrix[index2, index1] = 1 # if an edge has outdegree 0 , then considering it has edges to all the nodes from \n",
    "        # which it has incoming edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = [[] for _ in range(num_nodes)]\n",
    "for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "        if adj_matrix[i][j] == 1:\n",
    "            adj[i].append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "dist_matrix = np.full((num_nodes, num_nodes), np.inf, dtype=float)\n",
    "noofpaths = np.zeros((num_nodes, num_nodes), dtype=int)\n",
    "closeness_centrality_list = []\n",
    "\n",
    "for source_node in range(num_nodes):\n",
    "    # Reset distances for each source node\n",
    "    dist = [float('inf')] * num_nodes\n",
    "    q = Queue()\n",
    "    q.put((0, source_node))\n",
    "    dist[source_node] = 0\n",
    "    \n",
    "    noofpaths[source_node][source_node] = 1\n",
    "\n",
    "    while not q.empty():\n",
    "        dis, node = q.get()\n",
    "\n",
    "        for neighbor in adj[node]:\n",
    "            if dis + 1 < dist[neighbor]:\n",
    "                dist[neighbor] = dis + 1\n",
    "                q.put((dist[neighbor], neighbor))\n",
    "                noofpaths[neighbor][source_node] = noofpaths[source_node][node]\n",
    "            elif dis + 1 == dist[neighbor]:\n",
    "                noofpaths[neighbor][source_node] += noofpaths[source_node][node]    \n",
    "\n",
    "        # Calculate closeness centrality for the current source node\n",
    "    dist_matrix[source_node, :] = dist\n",
    "    valid_distances = [dist_i if dist_i != float('inf') else 1e9 for dist_i in dist] # taking 1e9 for unreachable nodes\n",
    "    sum_of_distances = sum(valid_distances)\n",
    "    closeness_centrality = (num_nodes - 1) / sum_of_distances if sum_of_distances != 0 else 0\n",
    "    closeness_centrality_list.append(closeness_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closeness Centrality scores have been sorted and written to 'closeness.txt'.\n"
     ]
    }
   ],
   "source": [
    "with open('temp1.txt', 'w') as file:\n",
    "    for index, score in enumerate(closeness_centrality_list):\n",
    "        file.write(f\"{node_indices[index]} {score:.12f}\\n\")\n",
    "\n",
    "with open('temp1.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Sort the lines based on the score\n",
    "sorted_lines = sorted(lines, key=lambda x: float(x.split(' ')[1]),reverse=True)\n",
    "\n",
    "# Write the sorted content back to the file\n",
    "with open('../../centralities/closeness.txt', 'w') as file:\n",
    "    file.writelines(sorted_lines)\n",
    "\n",
    "print(\"Closeness Centrality scores have been sorted and written to 'closeness.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = list(range(num_nodes))  # List of vertices\n",
    "\n",
    "# Create an empty adjacency list\n",
    "A = {vertex: [] for vertex in V}\n",
    "\n",
    "# Populate the adjacency list based on the adjacency matrix\n",
    "for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "        if adj_matrix[i][j] == 1:\n",
    "            A[i].append(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweenness Centrality\n",
    "\n",
    "Using Brandes Algorithm for faster betweenness centrality calculation with scaling factor 1/((n-1)*(n-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def brandes(V, A):\n",
    "    norm=((len(V)-1)*(len(V)-2)) # scaling factor 1/((n-1)*(n-2))\n",
    "    C = dict((v,0) for v in V)\n",
    "    for s in V:\n",
    "        S = []\n",
    "        P = dict((w,[]) for w in V)\n",
    "        g = dict((t, 0) for t in V); g[s] = 1\n",
    "        d = dict((t,-1) for t in V); d[s] = 0\n",
    "        Q = deque([])\n",
    "        Q.append(s)\n",
    "        while Q:\n",
    "            v = Q.popleft()\n",
    "            S.append(v)\n",
    "            for w in A[v]:\n",
    "                if d[w] < 0:\n",
    "                    Q.append(w)\n",
    "                    d[w] = d[v] + 1\n",
    "                if d[w] == d[v] + 1:\n",
    "                    g[w] = g[w] + g[v]\n",
    "                    P[w].append(v)\n",
    "        e = dict((v, 0) for v in V)\n",
    "        while S:\n",
    "            # print(s)\n",
    "            w = S.pop()\n",
    "            for v in P[w]:\n",
    "                e[v] = e[v] + ((g[v]/g[w]) * (1 + e[w]))\n",
    "            if w != s:\n",
    "                # print(w, e[w])\n",
    "                C[w] = C[w] + e[w]\n",
    "            \n",
    "    for v in V:\n",
    "        C[v] = C[v] /norm\n",
    "    return C\n",
    "\n",
    "betweenness_centrality_dic=brandes(V, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betweenness Centrality scores have been sorted and written to 'betweenness.txt'.\n"
     ]
    }
   ],
   "source": [
    "with open('temp2.txt', 'w') as file:\n",
    "    for node, score in betweenness_centrality_dic.items():\n",
    "        file.write(f\"{node_indices[node]} {score:.12f}\\n\")\n",
    "\n",
    "with open('temp2.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Sort the lines based on the score\n",
    "sorted_lines = sorted(lines, key=lambda x: float(x.split(' ')[1]), reverse=True)\n",
    "\n",
    "# Write the sorted content back to the file\n",
    "with open('../../centralities/betweenness.txt', 'w') as file:\n",
    "    file.writelines(sorted_lines)\n",
    "\n",
    "print(\"Betweenness Centrality scores have been sorted and written to 'betweenness.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_with_damping(adj_matrix, d=0.8, eps=1e-6, max_iter=100):\n",
    "    N = len(adj_matrix)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    num_nodes=N\n",
    "    # print(adj_matrix)\n",
    "    # Create the row-normalized version of the adjacency matrix\n",
    "    M = adj_matrix / adj_matrix.sum(axis=1, keepdims=True)\n",
    "    #print(M)\n",
    "    row_sums = np.sum(M, axis=1)\n",
    "    temp_matrix = np.ones((num_nodes, num_nodes), dtype=int)/num_nodes\n",
    "    M=(d)*M+(1-d)*temp_matrix\n",
    "    #print(M)\n",
    "    \n",
    "    # Initialize PageRank scores\n",
    "    R = np.ones(N) / N\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        R_new = np.dot(M.T, R)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(R_new - R, 1) < eps:\n",
    "            return R_new\n",
    "        \n",
    "        R = R_new\n",
    "    \n",
    "    return R\n",
    "\n",
    "pagerank_scores = pagerank_with_damping(adj_matrix)\n",
    "np.set_printoptions(suppress=True)\n",
    "with open('temp3.txt', 'w') as file:\n",
    "    for index, score in enumerate(pagerank_scores):\n",
    "        file.write(f\"{node_indices[index]} {score:.12f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank scores have been sorted and written to 'pagerank.txt'.\n"
     ]
    }
   ],
   "source": [
    "with open('temp3.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Sort the lines based on the score\n",
    "sorted_lines = sorted(lines, key=lambda x: float(x.split(' ')[1]),reverse=True)\n",
    "\n",
    "# Write the sorted content back to the file\n",
    "with open('../../centralities/pagerank.txt', 'w') as file:\n",
    "    file.writelines(sorted_lines)\n",
    "\n",
    "print(\"PageRank scores have been sorted and written to 'pagerank.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
